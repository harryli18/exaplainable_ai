# Analysis of Counterfactual Explanations via Regressions and Decision Trees

Explainable AI is a relatively new area of research in Machine Learning. The development of complex deep learning models led to an increase demand in finding methods to explain them. An interpretation method can be either local or global, which means it can be used for explaining an individual prediction or the entire model behaviour. In this thesis, we focus on local explanation. In particular, we study the effect of so-called counterfactual explanations, which gives the users an understanding of ‘what if the output had been different’. CLEAR and LORE are two recent state-of-art methods on providing counterfactual explanations. The authors of both methods demonstrated that both methods outperformed other mainstream techniques in term of fidelity and consistency with human intuition. This thesis continues the study of the effect of CLEAR and LORE by analysing the codes in greater detail, run case studies over different black-box models (MLP, Random Forest, SVM) and making suggestions to improve the algorithms. The two novel methods are compared and evaluated in four different aspect, decision boundary, fidelity, counterfactual explanations, and neighbourhood algorithms. Finally, this thesis provides suggestions on approaches that compare and evaluate many other interpretation methods. 

Author: Harry Li
